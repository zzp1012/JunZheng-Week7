{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras-LSTM\n",
    "## Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from py2neo import Graph, NodeMatcher, Relationship, Node, Record\n",
    "from pattern.en import singularize\n",
    "\n",
    "import tensorflow\n",
    "Sequential = tensorflow.keras.models.Sequential\n",
    "Dense = tensorflow.keras.layers.Dense\n",
    "LSTM = tensorflow.keras.layers.LSTM\n",
    "TimeDistributed = tensorflow.keras.layers.TimeDistributed\n",
    "RepeatVector = tensorflow.keras.layers.RepeatVector\n",
    "TensorBoard = tensorflow.keras.callbacks.TensorBoard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glove.6B.50d.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "word_vectors = dict()\n",
    "for line in lines:\n",
    "    line_split = line[:-1].split()\n",
    "    word_vectors[line_split[0]] = [float(number) for number in line_split[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"bolt://localhost:7687\", password = \"1012zzpHh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_nodes = graph.run(\"MATCH (paper:paper) RETURN paper.id, paper.title\").data()\n",
    "\n",
    "# Remove Punctuations\n",
    "for node in paper_nodes:\n",
    "    for ch in string.punctuation:\n",
    "        if ch is '-':\n",
    "            node['paper.title'] = node['paper.title'].replace(ch, ' ')\n",
    "        else:\n",
    "            node['paper.title'] = node['paper.title'].replace(ch, '')\n",
    "        \n",
    "\n",
    "# Lowercase Words\n",
    "for node in paper_nodes:\n",
    "    node['paper.title'] = [word.lower() for word in node['paper.title'].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['online', 'generation', 'of', 'profile', 'association', 'rules']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_nodes[15]['paper.title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Title Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "none_words = list()\n",
    "for node in paper_nodes:\n",
    "    title_vector = list()\n",
    "    for word in node['paper.title']:\n",
    "        try:\n",
    "            title_vector.append(word_vectors[word])\n",
    "        except KeyError:\n",
    "            try:\n",
    "                title_vector.append(word_vectors[singularize(word)])\n",
    "            except KeyError:\n",
    "                none_words.append(word)\n",
    "    node['title_vector'] = title_vector\n",
    "\n",
    "len(set(none_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word Vector for None_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "none_words_matrix = np.random.rand(273,50)*2 - 1\n",
    "none_word_vectors = dict()\n",
    "for i,word in enumerate(set(none_words)):\n",
    "    none_word_vectors[word] = none_words_matrix[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in paper_nodes:\n",
    "    for word in node['paper.title']:\n",
    "        if word in set(none_words):\n",
    "            node['title_vector'].append(none_word_vectors[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Title Matrix & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in paper_nodes:\n",
    "    node['title_vector'] = np.array(node['title_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(map(lambda x:x['title_vector'].shape[0],paper_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data_mat):\n",
    "    #centralize\n",
    "    data_mat = data_mat.T - np.mean(data_mat.T,axis = 0).reshape((1,data_mat.T.shape[1]))\n",
    "    # XX.T\n",
    "    cov_mat = np.cov(data_mat,rowvar = 0)\n",
    "    #eig\n",
    "    eigvals,eigvects = np.linalg.eig(np.mat(cov_mat))\n",
    "    eigvals_id = np.argsort(-eigvals)[:3]\n",
    "    eigvects_selected = eigvects[:,eigvals_id]\n",
    "    #reduce dims\n",
    "    reduced_data_mat = data_mat * eigvects_selected\n",
    "    return np.real(reduced_data_mat).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for node in paper_nodes:\n",
    "    node['title_vector'] = pca(node['title_vector'])\n",
    "paper_nodes[1]['title_vector'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987</td>\n",
       "      <td>1341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987</td>\n",
       "      <td>1412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987</td>\n",
       "      <td>2394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>316</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5962</th>\n",
       "      <td>1914</td>\n",
       "      <td>1748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963</th>\n",
       "      <td>1169</td>\n",
       "      <td>2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5964</th>\n",
       "      <td>1169</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5965</th>\n",
       "      <td>1169</td>\n",
       "      <td>1141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5966</th>\n",
       "      <td>1169</td>\n",
       "      <td>1702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5967 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      source  target\n",
       "0       1987    1341\n",
       "1       1987    1412\n",
       "2       1987    2394\n",
       "3        316     313\n",
       "4        316     318\n",
       "...      ...     ...\n",
       "5962    1914    1748\n",
       "5963    1169    2088\n",
       "5964    1169    1982\n",
       "5965    1169    1141\n",
       "5966    1169    1702\n",
       "\n",
       "[5967 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"citation_table.csv\",\"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "    lines = [line for line in lines if line is not '\\n']\n",
    "    citation_table = [[int(num) for num in line[0:-1].split(',')] for line in lines]\n",
    "citation_table = pd.DataFrame(citation_table,columns = [\"source\",'target'])\n",
    "citation_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper.id</th>\n",
       "      <th>paper.title</th>\n",
       "      <th>title_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987</td>\n",
       "      <td>[efficient, evaluation, of, aggregates, on, bu...</td>\n",
       "      <td>[[[[[-1.28749409  0.23764203  0.1963248   0.14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>316</td>\n",
       "      <td>[a, non, obtrusive, user, interface, for, incr...</td>\n",
       "      <td>[[[[[ 0.55718042  1.23926539  0.05190135  0.46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>358</td>\n",
       "      <td>[performance, of, dlp, on, random, modal, form...</td>\n",
       "      <td>[[[[[ 5.11703455e-01  3.93235971e-01 -5.772689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2227</td>\n",
       "      <td>[semcog, a, hybrid, object, based, image, data...</td>\n",
       "      <td>[[[[[ -1.1707025    0.15134707   0.06867833  -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2437</td>\n",
       "      <td>[the, use, of, information, capacity, in, sche...</td>\n",
       "      <td>[[[[[ 1.21138240e+00  6.64351211e-01 -1.746093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>157</td>\n",
       "      <td>[arguing, about, beliefs, and, actions]</td>\n",
       "      <td>[[[[[ 0.65251859 -0.30215619 -0.77729002 -0.69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>101</td>\n",
       "      <td>[characterization, of, database, access, skew,...</td>\n",
       "      <td>[[[[[-1.18638654 -0.05430245  0.09434683 -1.40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>395</td>\n",
       "      <td>[consistency, checking, in, complex, object, d...</td>\n",
       "      <td>[[[[[-5.56413959e-01 -5.35828955e-01  4.325371...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>1914</td>\n",
       "      <td>[queries, on, change, in, an, extended, relati...</td>\n",
       "      <td>[[[[[ 7.93680413e-01  9.39145473e-01 -3.787493...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2554</th>\n",
       "      <td>1169</td>\n",
       "      <td>[query, optimization, in, the, presence, of, f...</td>\n",
       "      <td>[[[[[-1.57548628e+00 -3.74760938e-01  8.947035...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2555 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      paper.id                                        paper.title  \\\n",
       "0         1987  [efficient, evaluation, of, aggregates, on, bu...   \n",
       "1          316  [a, non, obtrusive, user, interface, for, incr...   \n",
       "2          358  [performance, of, dlp, on, random, modal, form...   \n",
       "3         2227  [semcog, a, hybrid, object, based, image, data...   \n",
       "4         2437  [the, use, of, information, capacity, in, sche...   \n",
       "...        ...                                                ...   \n",
       "2550       157            [arguing, about, beliefs, and, actions]   \n",
       "2551       101  [characterization, of, database, access, skew,...   \n",
       "2552       395  [consistency, checking, in, complex, object, d...   \n",
       "2553      1914  [queries, on, change, in, an, extended, relati...   \n",
       "2554      1169  [query, optimization, in, the, presence, of, f...   \n",
       "\n",
       "                                           title_vector  \n",
       "0     [[[[[-1.28749409  0.23764203  0.1963248   0.14...  \n",
       "1     [[[[[ 0.55718042  1.23926539  0.05190135  0.46...  \n",
       "2     [[[[[ 5.11703455e-01  3.93235971e-01 -5.772689...  \n",
       "3     [[[[[ -1.1707025    0.15134707   0.06867833  -...  \n",
       "4     [[[[[ 1.21138240e+00  6.64351211e-01 -1.746093...  \n",
       "...                                                 ...  \n",
       "2550  [[[[[ 0.65251859 -0.30215619 -0.77729002 -0.69...  \n",
       "2551  [[[[[-1.18638654 -0.05430245  0.09434683 -1.40...  \n",
       "2552  [[[[[-5.56413959e-01 -5.35828955e-01  4.325371...  \n",
       "2553  [[[[[ 7.93680413e-01  9.39145473e-01 -3.787493...  \n",
       "2554  [[[[[-1.57548628e+00 -3.74760938e-01  8.947035...  \n",
       "\n",
       "[2555 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_table = pd.DataFrame(paper_nodes)\n",
    "paper_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 50)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_table[paper_table['paper.id'] == 1]['title_vector'].iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5967, 3, 50), (5967, 3, 50))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = list()\n",
    "train_y = list()\n",
    "for source, target in zip(citation_table['source'],citation_table['target']):\n",
    "    train_x.append(paper_table[paper_table['paper.id'] == source]['title_vector'].iloc[0])\n",
    "    train_y.append(paper_table[paper_table['paper.id'] == target]['title_vector'].iloc[0])\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X,Y):\n",
    "    np.random.seed(10)\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5967, 3, 50), (5967, 3, 50))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y = shuffle(train_x,train_y)\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data & Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,Y,val_rate):\n",
    "    X_train = X[int(X.shape[0]*(val_rate)):]\n",
    "    Y_train = Y[int(Y.shape[0]*(val_rate)):]\n",
    "    X_val = X[:int(X.shape[0]*val_rate)]\n",
    "    Y_val = Y[:int(Y.shape[0]*val_rate)]\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5371, 3, 50), (5371, 3, 50), (596, 3, 50), (596, 3, 50))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val= split_data(train_x, train_y,0.1)\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential() \n",
    "\n",
    "    model.add(LSTM(50, input_shape=(3, 50)))\n",
    "\n",
    "    model.add(RepeatVector(3))\n",
    "    \n",
    "    model.add(LSTM(25, return_sequences=True))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(50, activation= 'softmax' ))) \n",
    "\n",
    "    model.compile(loss='mse' , optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 3, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 3, 25)             7600      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 3, 50)             1300      \n",
      "=================================================================\n",
      "Total params: 29,100\n",
      "Trainable params: 29,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallBack = TensorBoard(log_dir='logs\\\\final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5371 samples, validate on 596 samples\n",
      "Epoch 1/300\n",
      " 100/5371 [..............................] - ETA: 1:46 - loss: 1.1997 - accuracy: 0.0133WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.102372). Check your callbacks.\n",
      "5371/5371 [==============================] - 3s 568us/sample - loss: 1.1516 - accuracy: 0.0148 - val_loss: 1.1608 - val_accuracy: 0.0252\n",
      "Epoch 2/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1515 - accuracy: 0.0395 - val_loss: 1.1605 - val_accuracy: 0.0503\n",
      "Epoch 3/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1509 - accuracy: 0.0464 - val_loss: 1.1595 - val_accuracy: 0.0716\n",
      "Epoch 4/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1504 - accuracy: 0.0549 - val_loss: 1.1590 - val_accuracy: 0.0436\n",
      "Epoch 5/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1500 - accuracy: 0.0580 - val_loss: 1.1587 - val_accuracy: 0.0587\n",
      "Epoch 6/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1495 - accuracy: 0.1285 - val_loss: 1.1585 - val_accuracy: 0.1566\n",
      "Epoch 7/300\n",
      "5371/5371 [==============================] - 0s 64us/sample - loss: 1.1491 - accuracy: 0.1499 - val_loss: 1.1582 - val_accuracy: 0.1577\n",
      "Epoch 8/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1488 - accuracy: 0.1552 - val_loss: 1.1586 - val_accuracy: 0.1560\n",
      "Epoch 9/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1486 - accuracy: 0.1534 - val_loss: 1.1580 - val_accuracy: 0.1499\n",
      "Epoch 10/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1484 - accuracy: 0.1509 - val_loss: 1.1583 - val_accuracy: 0.1477\n",
      "Epoch 11/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1481 - accuracy: 0.1504 - val_loss: 1.1582 - val_accuracy: 0.1460\n",
      "Epoch 12/300\n",
      "5371/5371 [==============================] - 0s 79us/sample - loss: 1.1478 - accuracy: 0.1506 - val_loss: 1.1588 - val_accuracy: 0.1393\n",
      "Epoch 13/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1474 - accuracy: 0.1664 - val_loss: 1.1585 - val_accuracy: 0.1460\n",
      "Epoch 14/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1466 - accuracy: 0.1794 - val_loss: 1.1609 - val_accuracy: 0.1544\n",
      "Epoch 15/300\n",
      "5371/5371 [==============================] - 0s 75us/sample - loss: 1.1447 - accuracy: 0.1837 - val_loss: 1.1620 - val_accuracy: 0.1538\n",
      "Epoch 16/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1429 - accuracy: 0.1872 - val_loss: 1.1650 - val_accuracy: 0.1560\n",
      "Epoch 17/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1422 - accuracy: 0.1897 - val_loss: 1.1637 - val_accuracy: 0.1588\n",
      "Epoch 18/300\n",
      "5371/5371 [==============================] - 0s 73us/sample - loss: 1.1410 - accuracy: 0.1918 - val_loss: 1.1642 - val_accuracy: 0.1482\n",
      "Epoch 19/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1403 - accuracy: 0.1949 - val_loss: 1.1653 - val_accuracy: 0.1493\n",
      "Epoch 20/300\n",
      "5371/5371 [==============================] - 1s 111us/sample - loss: 1.1395 - accuracy: 0.1927 - val_loss: 1.1651 - val_accuracy: 0.1566\n",
      "Epoch 21/300\n",
      "5371/5371 [==============================] - 1s 212us/sample - loss: 1.1391 - accuracy: 0.1944 - val_loss: 1.1649 - val_accuracy: 0.1460\n",
      "Epoch 22/300\n",
      "5371/5371 [==============================] - 0s 74us/sample - loss: 1.1384 - accuracy: 0.1964 - val_loss: 1.1628 - val_accuracy: 0.1527\n",
      "Epoch 23/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1381 - accuracy: 0.1974 - val_loss: 1.1638 - val_accuracy: 0.1488\n",
      "Epoch 24/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1379 - accuracy: 0.1972 - val_loss: 1.1649 - val_accuracy: 0.1443\n",
      "Epoch 25/300\n",
      "5371/5371 [==============================] - 0s 74us/sample - loss: 1.1376 - accuracy: 0.1969 - val_loss: 1.1637 - val_accuracy: 0.1504\n",
      "Epoch 26/300\n",
      "5371/5371 [==============================] - 0s 76us/sample - loss: 1.1372 - accuracy: 0.2009 - val_loss: 1.1661 - val_accuracy: 0.1493\n",
      "Epoch 27/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1372 - accuracy: 0.2005 - val_loss: 1.1641 - val_accuracy: 0.1555\n",
      "Epoch 28/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1370 - accuracy: 0.1985 - val_loss: 1.1650 - val_accuracy: 0.1532\n",
      "Epoch 29/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1366 - accuracy: 0.2013 - val_loss: 1.1640 - val_accuracy: 0.1477\n",
      "Epoch 30/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1363 - accuracy: 0.1995 - val_loss: 1.1652 - val_accuracy: 0.1572\n",
      "Epoch 31/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1362 - accuracy: 0.2016 - val_loss: 1.1635 - val_accuracy: 0.1611\n",
      "Epoch 32/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1360 - accuracy: 0.1993 - val_loss: 1.1657 - val_accuracy: 0.1477\n",
      "Epoch 33/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1357 - accuracy: 0.2010 - val_loss: 1.1652 - val_accuracy: 0.1499\n",
      "Epoch 34/300\n",
      "5371/5371 [==============================] - 0s 73us/sample - loss: 1.1354 - accuracy: 0.2006 - val_loss: 1.1655 - val_accuracy: 0.1510\n",
      "Epoch 35/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1354 - accuracy: 0.2044 - val_loss: 1.1641 - val_accuracy: 0.1521\n",
      "Epoch 36/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1353 - accuracy: 0.2025 - val_loss: 1.1642 - val_accuracy: 0.1477\n",
      "Epoch 37/300\n",
      "5371/5371 [==============================] - 0s 76us/sample - loss: 1.1350 - accuracy: 0.1991 - val_loss: 1.1653 - val_accuracy: 0.1454\n",
      "Epoch 38/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1348 - accuracy: 0.2010 - val_loss: 1.1653 - val_accuracy: 0.1493\n",
      "Epoch 39/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1345 - accuracy: 0.2029 - val_loss: 1.1651 - val_accuracy: 0.1454\n",
      "Epoch 40/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1343 - accuracy: 0.2029 - val_loss: 1.1649 - val_accuracy: 0.1432\n",
      "Epoch 41/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1342 - accuracy: 0.2012 - val_loss: 1.1651 - val_accuracy: 0.1437\n",
      "Epoch 42/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1339 - accuracy: 0.2037 - val_loss: 1.1653 - val_accuracy: 0.1449\n",
      "Epoch 43/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1338 - accuracy: 0.2031 - val_loss: 1.1657 - val_accuracy: 0.1449\n",
      "Epoch 44/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1338 - accuracy: 0.2032 - val_loss: 1.1644 - val_accuracy: 0.1465\n",
      "Epoch 45/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1335 - accuracy: 0.2039 - val_loss: 1.1636 - val_accuracy: 0.1510\n",
      "Epoch 46/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1339 - accuracy: 0.2038 - val_loss: 1.1655 - val_accuracy: 0.1432\n",
      "Epoch 47/300\n",
      "5371/5371 [==============================] - 0s 73us/sample - loss: 1.1336 - accuracy: 0.2054 - val_loss: 1.1645 - val_accuracy: 0.1504\n",
      "Epoch 48/300\n",
      "5371/5371 [==============================] - 0s 70us/sample - loss: 1.1333 - accuracy: 0.2042 - val_loss: 1.1650 - val_accuracy: 0.1348\n",
      "Epoch 49/300\n",
      "5371/5371 [==============================] - 0s 81us/sample - loss: 1.1330 - accuracy: 0.2044 - val_loss: 1.1652 - val_accuracy: 0.1359\n",
      "Epoch 50/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1329 - accuracy: 0.2057 - val_loss: 1.1656 - val_accuracy: 0.1381\n",
      "Epoch 51/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1328 - accuracy: 0.2056 - val_loss: 1.1656 - val_accuracy: 0.1376\n",
      "Epoch 52/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1327 - accuracy: 0.2046 - val_loss: 1.1650 - val_accuracy: 0.1432\n",
      "Epoch 53/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1328 - accuracy: 0.2044 - val_loss: 1.1652 - val_accuracy: 0.1398\n",
      "Epoch 54/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1325 - accuracy: 0.2065 - val_loss: 1.1653 - val_accuracy: 0.1376\n",
      "Epoch 55/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1323 - accuracy: 0.2051 - val_loss: 1.1652 - val_accuracy: 0.1421\n",
      "Epoch 56/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1320 - accuracy: 0.2080 - val_loss: 1.1655 - val_accuracy: 0.1365\n",
      "Epoch 57/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1319 - accuracy: 0.2062 - val_loss: 1.1656 - val_accuracy: 0.1415\n",
      "Epoch 58/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1318 - accuracy: 0.2069 - val_loss: 1.1665 - val_accuracy: 0.1359\n",
      "Epoch 59/300\n",
      "5371/5371 [==============================] - 0s 62us/sample - loss: 1.1316 - accuracy: 0.2078 - val_loss: 1.1662 - val_accuracy: 0.1342\n",
      "Epoch 60/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1315 - accuracy: 0.2087 - val_loss: 1.1660 - val_accuracy: 0.1381\n",
      "Epoch 61/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1318 - accuracy: 0.2061 - val_loss: 1.1665 - val_accuracy: 0.1331\n",
      "Epoch 62/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1318 - accuracy: 0.2069 - val_loss: 1.1665 - val_accuracy: 0.1454\n",
      "Epoch 63/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1313 - accuracy: 0.2077 - val_loss: 1.1659 - val_accuracy: 0.1381\n",
      "Epoch 64/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1310 - accuracy: 0.2103 - val_loss: 1.1663 - val_accuracy: 0.1421\n",
      "Epoch 65/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1311 - accuracy: 0.2102 - val_loss: 1.1665 - val_accuracy: 0.1370\n",
      "Epoch 66/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1308 - accuracy: 0.2114 - val_loss: 1.1667 - val_accuracy: 0.1348\n",
      "Epoch 67/300\n",
      "5371/5371 [==============================] - 0s 62us/sample - loss: 1.1306 - accuracy: 0.2103 - val_loss: 1.1667 - val_accuracy: 0.1398\n",
      "Epoch 68/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1304 - accuracy: 0.2132 - val_loss: 1.1664 - val_accuracy: 0.1426\n",
      "Epoch 69/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1301 - accuracy: 0.2143 - val_loss: 1.1671 - val_accuracy: 0.1326\n",
      "Epoch 70/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1301 - accuracy: 0.2137 - val_loss: 1.1673 - val_accuracy: 0.1443\n",
      "Epoch 71/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1300 - accuracy: 0.2132 - val_loss: 1.1666 - val_accuracy: 0.1409\n",
      "Epoch 72/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1298 - accuracy: 0.2144 - val_loss: 1.1669 - val_accuracy: 0.1359\n",
      "Epoch 73/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1298 - accuracy: 0.2131 - val_loss: 1.1675 - val_accuracy: 0.1370\n",
      "Epoch 74/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1296 - accuracy: 0.2136 - val_loss: 1.1676 - val_accuracy: 0.1353\n",
      "Epoch 75/300\n",
      "5371/5371 [==============================] - 0s 70us/sample - loss: 1.1295 - accuracy: 0.2150 - val_loss: 1.1676 - val_accuracy: 0.1393\n",
      "Epoch 76/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1294 - accuracy: 0.2172 - val_loss: 1.1679 - val_accuracy: 0.1365\n",
      "Epoch 77/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1293 - accuracy: 0.2162 - val_loss: 1.1675 - val_accuracy: 0.1292\n",
      "Epoch 78/300\n",
      "5371/5371 [==============================] - 0s 70us/sample - loss: 1.1292 - accuracy: 0.2161 - val_loss: 1.1676 - val_accuracy: 0.1359\n",
      "Epoch 79/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1291 - accuracy: 0.2154 - val_loss: 1.1681 - val_accuracy: 0.1426\n",
      "Epoch 80/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1291 - accuracy: 0.2165 - val_loss: 1.1680 - val_accuracy: 0.1393\n",
      "Epoch 81/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1290 - accuracy: 0.2184 - val_loss: 1.1680 - val_accuracy: 0.1309\n",
      "Epoch 82/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1288 - accuracy: 0.2168 - val_loss: 1.1681 - val_accuracy: 0.1353\n",
      "Epoch 83/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1288 - accuracy: 0.2158 - val_loss: 1.1681 - val_accuracy: 0.1326\n",
      "Epoch 84/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1287 - accuracy: 0.2195 - val_loss: 1.1678 - val_accuracy: 0.1337\n",
      "Epoch 85/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1287 - accuracy: 0.2168 - val_loss: 1.1683 - val_accuracy: 0.1292\n",
      "Epoch 86/300\n",
      "5371/5371 [==============================] - 1s 121us/sample - loss: 1.1293 - accuracy: 0.2151 - val_loss: 1.1672 - val_accuracy: 0.1326\n",
      "Epoch 87/300\n",
      "5371/5371 [==============================] - 1s 206us/sample - loss: 1.1289 - accuracy: 0.2184 - val_loss: 1.1685 - val_accuracy: 0.1331\n",
      "Epoch 88/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1291 - accuracy: 0.2166 - val_loss: 1.1690 - val_accuracy: 0.1286\n",
      "Epoch 89/300\n",
      "5371/5371 [==============================] - 0s 64us/sample - loss: 1.1289 - accuracy: 0.2170 - val_loss: 1.1691 - val_accuracy: 0.1230\n",
      "Epoch 90/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1287 - accuracy: 0.2183 - val_loss: 1.1687 - val_accuracy: 0.1264\n",
      "Epoch 91/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1283 - accuracy: 0.2174 - val_loss: 1.1683 - val_accuracy: 0.1326\n",
      "Epoch 92/300\n",
      "5371/5371 [==============================] - 0s 74us/sample - loss: 1.1284 - accuracy: 0.2178 - val_loss: 1.1682 - val_accuracy: 0.1258\n",
      "Epoch 93/300\n",
      "5371/5371 [==============================] - 0s 76us/sample - loss: 1.1281 - accuracy: 0.2176 - val_loss: 1.1680 - val_accuracy: 0.1286\n",
      "Epoch 94/300\n",
      "5371/5371 [==============================] - 0s 75us/sample - loss: 1.1279 - accuracy: 0.2177 - val_loss: 1.1683 - val_accuracy: 0.1270\n",
      "Epoch 95/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1280 - accuracy: 0.2188 - val_loss: 1.1686 - val_accuracy: 0.1258\n",
      "Epoch 96/300\n",
      "5371/5371 [==============================] - 0s 77us/sample - loss: 1.1278 - accuracy: 0.2206 - val_loss: 1.1686 - val_accuracy: 0.1286\n",
      "Epoch 97/300\n",
      "5371/5371 [==============================] - 0s 70us/sample - loss: 1.1278 - accuracy: 0.2211 - val_loss: 1.1684 - val_accuracy: 0.1242\n",
      "Epoch 98/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1277 - accuracy: 0.2191 - val_loss: 1.1685 - val_accuracy: 0.1326\n",
      "Epoch 99/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1279 - accuracy: 0.2216 - val_loss: 1.1683 - val_accuracy: 0.1219\n",
      "Epoch 100/300\n",
      "5371/5371 [==============================] - 0s 74us/sample - loss: 1.1278 - accuracy: 0.2195 - val_loss: 1.1688 - val_accuracy: 0.1281\n",
      "Epoch 101/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1276 - accuracy: 0.2211 - val_loss: 1.1685 - val_accuracy: 0.1242\n",
      "Epoch 102/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1274 - accuracy: 0.2217 - val_loss: 1.1684 - val_accuracy: 0.1225\n",
      "Epoch 103/300\n",
      "5371/5371 [==============================] - 0s 77us/sample - loss: 1.1275 - accuracy: 0.2178 - val_loss: 1.1688 - val_accuracy: 0.1264\n",
      "Epoch 104/300\n",
      "5371/5371 [==============================] - 0s 73us/sample - loss: 1.1274 - accuracy: 0.2205 - val_loss: 1.1685 - val_accuracy: 0.1258\n",
      "Epoch 105/300\n",
      "5371/5371 [==============================] - 0s 70us/sample - loss: 1.1273 - accuracy: 0.2197 - val_loss: 1.1689 - val_accuracy: 0.1253\n",
      "Epoch 106/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1272 - accuracy: 0.2218 - val_loss: 1.1687 - val_accuracy: 0.1292\n",
      "Epoch 107/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1271 - accuracy: 0.2230 - val_loss: 1.1690 - val_accuracy: 0.1214\n",
      "Epoch 108/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1271 - accuracy: 0.2206 - val_loss: 1.1687 - val_accuracy: 0.1253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1270 - accuracy: 0.2215 - val_loss: 1.1689 - val_accuracy: 0.1236\n",
      "Epoch 110/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1270 - accuracy: 0.2205 - val_loss: 1.1689 - val_accuracy: 0.1303\n",
      "Epoch 111/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1269 - accuracy: 0.2221 - val_loss: 1.1689 - val_accuracy: 0.1202\n",
      "Epoch 112/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1269 - accuracy: 0.2237 - val_loss: 1.1688 - val_accuracy: 0.1197\n",
      "Epoch 113/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1268 - accuracy: 0.2221 - val_loss: 1.1686 - val_accuracy: 0.1174\n",
      "Epoch 114/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1268 - accuracy: 0.2214 - val_loss: 1.1689 - val_accuracy: 0.1236\n",
      "Epoch 115/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1267 - accuracy: 0.2226 - val_loss: 1.1684 - val_accuracy: 0.1230\n",
      "Epoch 116/300\n",
      "5371/5371 [==============================] - 0s 78us/sample - loss: 1.1267 - accuracy: 0.2241 - val_loss: 1.1688 - val_accuracy: 0.1225\n",
      "Epoch 117/300\n",
      "5371/5371 [==============================] - 0s 74us/sample - loss: 1.1266 - accuracy: 0.2226 - val_loss: 1.1689 - val_accuracy: 0.1253\n",
      "Epoch 118/300\n",
      "5371/5371 [==============================] - 0s 76us/sample - loss: 1.1265 - accuracy: 0.2239 - val_loss: 1.1686 - val_accuracy: 0.1258\n",
      "Epoch 119/300\n",
      "5371/5371 [==============================] - 0s 70us/sample - loss: 1.1265 - accuracy: 0.2253 - val_loss: 1.1687 - val_accuracy: 0.1197\n",
      "Epoch 120/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1265 - accuracy: 0.2244 - val_loss: 1.1681 - val_accuracy: 0.1163\n",
      "Epoch 121/300\n",
      "5371/5371 [==============================] - 0s 79us/sample - loss: 1.1264 - accuracy: 0.2219 - val_loss: 1.1690 - val_accuracy: 0.1270\n",
      "Epoch 122/300\n",
      "5371/5371 [==============================] - 0s 74us/sample - loss: 1.1264 - accuracy: 0.2245 - val_loss: 1.1688 - val_accuracy: 0.1253\n",
      "Epoch 123/300\n",
      "5371/5371 [==============================] - 0s 75us/sample - loss: 1.1265 - accuracy: 0.2237 - val_loss: 1.1688 - val_accuracy: 0.1225\n",
      "Epoch 124/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1264 - accuracy: 0.2250 - val_loss: 1.1684 - val_accuracy: 0.1275\n",
      "Epoch 125/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1262 - accuracy: 0.2258 - val_loss: 1.1685 - val_accuracy: 0.1236\n",
      "Epoch 126/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1262 - accuracy: 0.2237 - val_loss: 1.1683 - val_accuracy: 0.1264\n",
      "Epoch 127/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1261 - accuracy: 0.2248 - val_loss: 1.1686 - val_accuracy: 0.1275\n",
      "Epoch 128/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1262 - accuracy: 0.2275 - val_loss: 1.1681 - val_accuracy: 0.1214\n",
      "Epoch 129/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1261 - accuracy: 0.2253 - val_loss: 1.1688 - val_accuracy: 0.1270\n",
      "Epoch 130/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1260 - accuracy: 0.2271 - val_loss: 1.1686 - val_accuracy: 0.1225\n",
      "Epoch 131/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1260 - accuracy: 0.2252 - val_loss: 1.1684 - val_accuracy: 0.1275\n",
      "Epoch 132/300\n",
      "5371/5371 [==============================] - 0s 75us/sample - loss: 1.1259 - accuracy: 0.2270 - val_loss: 1.1685 - val_accuracy: 0.1230\n",
      "Epoch 133/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1259 - accuracy: 0.2268 - val_loss: 1.1686 - val_accuracy: 0.1253\n",
      "Epoch 134/300\n",
      "5371/5371 [==============================] - 0s 70us/sample - loss: 1.1258 - accuracy: 0.2275 - val_loss: 1.1686 - val_accuracy: 0.1253\n",
      "Epoch 135/300\n",
      "5371/5371 [==============================] - 0s 84us/sample - loss: 1.1259 - accuracy: 0.2276 - val_loss: 1.1690 - val_accuracy: 0.1247\n",
      "Epoch 136/300\n",
      "5371/5371 [==============================] - 0s 76us/sample - loss: 1.1258 - accuracy: 0.2264 - val_loss: 1.1690 - val_accuracy: 0.1242\n",
      "Epoch 137/300\n",
      "5371/5371 [==============================] - 0s 64us/sample - loss: 1.1257 - accuracy: 0.2281 - val_loss: 1.1688 - val_accuracy: 0.1230\n",
      "Epoch 138/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1256 - accuracy: 0.2285 - val_loss: 1.1691 - val_accuracy: 0.1214\n",
      "Epoch 139/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1256 - accuracy: 0.2288 - val_loss: 1.1691 - val_accuracy: 0.1230\n",
      "Epoch 140/300\n",
      "5371/5371 [==============================] - 0s 70us/sample - loss: 1.1255 - accuracy: 0.2295 - val_loss: 1.1689 - val_accuracy: 0.1247\n",
      "Epoch 141/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1255 - accuracy: 0.2287 - val_loss: 1.1690 - val_accuracy: 0.1202\n",
      "Epoch 142/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1255 - accuracy: 0.2288 - val_loss: 1.1690 - val_accuracy: 0.1275\n",
      "Epoch 143/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1254 - accuracy: 0.2283 - val_loss: 1.1691 - val_accuracy: 0.1208\n",
      "Epoch 144/300\n",
      "5371/5371 [==============================] - 0s 76us/sample - loss: 1.1254 - accuracy: 0.2298 - val_loss: 1.1690 - val_accuracy: 0.1225\n",
      "Epoch 145/300\n",
      "5371/5371 [==============================] - 0s 70us/sample - loss: 1.1253 - accuracy: 0.2298 - val_loss: 1.1691 - val_accuracy: 0.1242\n",
      "Epoch 146/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1253 - accuracy: 0.2306 - val_loss: 1.1691 - val_accuracy: 0.1208\n",
      "Epoch 147/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1252 - accuracy: 0.2301 - val_loss: 1.1692 - val_accuracy: 0.1247\n",
      "Epoch 148/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1253 - accuracy: 0.2302 - val_loss: 1.1690 - val_accuracy: 0.1236\n",
      "Epoch 149/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1254 - accuracy: 0.2295 - val_loss: 1.1687 - val_accuracy: 0.1236\n",
      "Epoch 150/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1254 - accuracy: 0.2328 - val_loss: 1.1688 - val_accuracy: 0.1208\n",
      "Epoch 151/300\n",
      "5371/5371 [==============================] - 0s 64us/sample - loss: 1.1251 - accuracy: 0.2312 - val_loss: 1.1690 - val_accuracy: 0.1230\n",
      "Epoch 152/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1251 - accuracy: 0.2316 - val_loss: 1.1691 - val_accuracy: 0.1214\n",
      "Epoch 153/300\n",
      "5371/5371 [==============================] - 1s 258us/sample - loss: 1.1250 - accuracy: 0.2316 - val_loss: 1.1687 - val_accuracy: 0.1214\n",
      "Epoch 154/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1250 - accuracy: 0.2319 - val_loss: 1.1689 - val_accuracy: 0.1236\n",
      "Epoch 155/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1250 - accuracy: 0.2296 - val_loss: 1.1685 - val_accuracy: 0.1258\n",
      "Epoch 156/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1251 - accuracy: 0.2327 - val_loss: 1.1684 - val_accuracy: 0.1197\n",
      "Epoch 157/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1249 - accuracy: 0.2327 - val_loss: 1.1687 - val_accuracy: 0.1214\n",
      "Epoch 158/300\n",
      "5371/5371 [==============================] - 0s 75us/sample - loss: 1.1249 - accuracy: 0.2317 - val_loss: 1.1686 - val_accuracy: 0.1253\n",
      "Epoch 159/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1247 - accuracy: 0.2335 - val_loss: 1.1684 - val_accuracy: 0.1258\n",
      "Epoch 160/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1247 - accuracy: 0.2330 - val_loss: 1.1685 - val_accuracy: 0.1219\n",
      "Epoch 161/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1246 - accuracy: 0.2330 - val_loss: 1.1685 - val_accuracy: 0.1247\n",
      "Epoch 162/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1246 - accuracy: 0.2340 - val_loss: 1.1684 - val_accuracy: 0.1202\n",
      "Epoch 163/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5371/5371 [==============================] - 0s 62us/sample - loss: 1.1246 - accuracy: 0.2334 - val_loss: 1.1684 - val_accuracy: 0.1197\n",
      "Epoch 164/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1246 - accuracy: 0.2334 - val_loss: 1.1684 - val_accuracy: 0.1236\n",
      "Epoch 165/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1245 - accuracy: 0.2345 - val_loss: 1.1687 - val_accuracy: 0.1197\n",
      "Epoch 166/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1246 - accuracy: 0.2339 - val_loss: 1.1683 - val_accuracy: 0.1186\n",
      "Epoch 167/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1245 - accuracy: 0.2334 - val_loss: 1.1687 - val_accuracy: 0.1270\n",
      "Epoch 168/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1245 - accuracy: 0.2343 - val_loss: 1.1687 - val_accuracy: 0.1236\n",
      "Epoch 169/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1245 - accuracy: 0.2323 - val_loss: 1.1686 - val_accuracy: 0.1264\n",
      "Epoch 170/300\n",
      "5371/5371 [==============================] - 0s 64us/sample - loss: 1.1245 - accuracy: 0.2331 - val_loss: 1.1684 - val_accuracy: 0.1230\n",
      "Epoch 171/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1244 - accuracy: 0.2327 - val_loss: 1.1686 - val_accuracy: 0.1270\n",
      "Epoch 172/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1244 - accuracy: 0.2326 - val_loss: 1.1685 - val_accuracy: 0.1214\n",
      "Epoch 173/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1243 - accuracy: 0.2330 - val_loss: 1.1685 - val_accuracy: 0.1242\n",
      "Epoch 174/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1244 - accuracy: 0.2356 - val_loss: 1.1678 - val_accuracy: 0.1236\n",
      "Epoch 175/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1242 - accuracy: 0.2328 - val_loss: 1.1684 - val_accuracy: 0.1247\n",
      "Epoch 176/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1242 - accuracy: 0.2347 - val_loss: 1.1681 - val_accuracy: 0.1264\n",
      "Epoch 177/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1243 - accuracy: 0.2345 - val_loss: 1.1685 - val_accuracy: 0.1247\n",
      "Epoch 178/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1242 - accuracy: 0.2348 - val_loss: 1.1681 - val_accuracy: 0.1264\n",
      "Epoch 179/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1241 - accuracy: 0.2349 - val_loss: 1.1683 - val_accuracy: 0.1275\n",
      "Epoch 180/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1240 - accuracy: 0.2356 - val_loss: 1.1682 - val_accuracy: 0.1270\n",
      "Epoch 181/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1239 - accuracy: 0.2376 - val_loss: 1.1685 - val_accuracy: 0.1225\n",
      "Epoch 182/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1239 - accuracy: 0.2378 - val_loss: 1.1682 - val_accuracy: 0.1242\n",
      "Epoch 183/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1239 - accuracy: 0.2378 - val_loss: 1.1684 - val_accuracy: 0.1236\n",
      "Epoch 184/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1238 - accuracy: 0.2386 - val_loss: 1.1684 - val_accuracy: 0.1230\n",
      "Epoch 185/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1238 - accuracy: 0.2370 - val_loss: 1.1683 - val_accuracy: 0.1225\n",
      "Epoch 186/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1238 - accuracy: 0.2377 - val_loss: 1.1684 - val_accuracy: 0.1214\n",
      "Epoch 187/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1239 - accuracy: 0.2378 - val_loss: 1.1684 - val_accuracy: 0.1236\n",
      "Epoch 188/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1247 - accuracy: 0.2319 - val_loss: 1.1688 - val_accuracy: 0.1264\n",
      "Epoch 189/300\n",
      "5371/5371 [==============================] - 0s 75us/sample - loss: 1.1263 - accuracy: 0.2302 - val_loss: 1.1680 - val_accuracy: 0.1230\n",
      "Epoch 190/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1252 - accuracy: 0.2348 - val_loss: 1.1687 - val_accuracy: 0.1253\n",
      "Epoch 191/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1251 - accuracy: 0.2342 - val_loss: 1.1691 - val_accuracy: 0.1191\n",
      "Epoch 192/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1249 - accuracy: 0.2342 - val_loss: 1.1684 - val_accuracy: 0.1258\n",
      "Epoch 193/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1246 - accuracy: 0.2355 - val_loss: 1.1685 - val_accuracy: 0.1242\n",
      "Epoch 194/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1243 - accuracy: 0.2352 - val_loss: 1.1689 - val_accuracy: 0.1230\n",
      "Epoch 195/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1239 - accuracy: 0.2399 - val_loss: 1.1685 - val_accuracy: 0.1208\n",
      "Epoch 196/300\n",
      "5371/5371 [==============================] - 0s 81us/sample - loss: 1.1241 - accuracy: 0.2366 - val_loss: 1.1686 - val_accuracy: 0.1270\n",
      "Epoch 197/300\n",
      "5371/5371 [==============================] - 0s 75us/sample - loss: 1.1239 - accuracy: 0.2376 - val_loss: 1.1684 - val_accuracy: 0.1270\n",
      "Epoch 198/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1238 - accuracy: 0.2409 - val_loss: 1.1682 - val_accuracy: 0.1236\n",
      "Epoch 199/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1238 - accuracy: 0.2397 - val_loss: 1.1686 - val_accuracy: 0.1242\n",
      "Epoch 200/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1238 - accuracy: 0.2394 - val_loss: 1.1685 - val_accuracy: 0.1230\n",
      "Epoch 201/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1236 - accuracy: 0.2423 - val_loss: 1.1681 - val_accuracy: 0.1230\n",
      "Epoch 202/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1236 - accuracy: 0.2402 - val_loss: 1.1679 - val_accuracy: 0.1225\n",
      "Epoch 203/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1236 - accuracy: 0.2391 - val_loss: 1.1676 - val_accuracy: 0.1303\n",
      "Epoch 204/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1235 - accuracy: 0.2414 - val_loss: 1.1680 - val_accuracy: 0.1225\n",
      "Epoch 205/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1233 - accuracy: 0.2416 - val_loss: 1.1680 - val_accuracy: 0.1258\n",
      "Epoch 206/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1233 - accuracy: 0.2412 - val_loss: 1.1680 - val_accuracy: 0.1270\n",
      "Epoch 207/300\n",
      "5371/5371 [==============================] - 0s 68us/sample - loss: 1.1233 - accuracy: 0.2427 - val_loss: 1.1678 - val_accuracy: 0.1230\n",
      "Epoch 208/300\n",
      "5371/5371 [==============================] - 0s 74us/sample - loss: 1.1232 - accuracy: 0.2421 - val_loss: 1.1680 - val_accuracy: 0.1225\n",
      "Epoch 209/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1233 - accuracy: 0.2414 - val_loss: 1.1679 - val_accuracy: 0.1264\n",
      "Epoch 210/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1232 - accuracy: 0.2437 - val_loss: 1.1680 - val_accuracy: 0.1230\n",
      "Epoch 211/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1232 - accuracy: 0.2425 - val_loss: 1.1681 - val_accuracy: 0.1264\n",
      "Epoch 212/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1231 - accuracy: 0.2442 - val_loss: 1.1682 - val_accuracy: 0.1253\n",
      "Epoch 213/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1231 - accuracy: 0.2437 - val_loss: 1.1680 - val_accuracy: 0.1264\n",
      "Epoch 214/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1231 - accuracy: 0.2432 - val_loss: 1.1683 - val_accuracy: 0.1247\n",
      "Epoch 215/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1231 - accuracy: 0.2433 - val_loss: 1.1678 - val_accuracy: 0.1225\n",
      "Epoch 216/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1231 - accuracy: 0.2432 - val_loss: 1.1678 - val_accuracy: 0.1253\n",
      "Epoch 217/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1231 - accuracy: 0.2437 - val_loss: 1.1678 - val_accuracy: 0.1253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218/300\n",
      "5371/5371 [==============================] - 1s 108us/sample - loss: 1.1231 - accuracy: 0.2435 - val_loss: 1.1683 - val_accuracy: 0.1270\n",
      "Epoch 219/300\n",
      "5371/5371 [==============================] - 1s 212us/sample - loss: 1.1231 - accuracy: 0.2426 - val_loss: 1.1679 - val_accuracy: 0.1281\n",
      "Epoch 220/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1230 - accuracy: 0.2450 - val_loss: 1.1680 - val_accuracy: 0.1258\n",
      "Epoch 221/300\n",
      "5371/5371 [==============================] - 0s 78us/sample - loss: 1.1230 - accuracy: 0.2422 - val_loss: 1.1679 - val_accuracy: 0.1281\n",
      "Epoch 222/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1230 - accuracy: 0.2446 - val_loss: 1.1676 - val_accuracy: 0.1264\n",
      "Epoch 223/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1231 - accuracy: 0.2442 - val_loss: 1.1677 - val_accuracy: 0.1281\n",
      "Epoch 224/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1230 - accuracy: 0.2445 - val_loss: 1.1680 - val_accuracy: 0.1303\n",
      "Epoch 225/300\n",
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1230 - accuracy: 0.2440 - val_loss: 1.1678 - val_accuracy: 0.1309\n",
      "Epoch 226/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1229 - accuracy: 0.2446 - val_loss: 1.1679 - val_accuracy: 0.1258\n",
      "Epoch 227/300\n",
      "5371/5371 [==============================] - 0s 74us/sample - loss: 1.1229 - accuracy: 0.2439 - val_loss: 1.1677 - val_accuracy: 0.1314\n",
      "Epoch 228/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1229 - accuracy: 0.2438 - val_loss: 1.1678 - val_accuracy: 0.1286\n",
      "Epoch 229/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1228 - accuracy: 0.2458 - val_loss: 1.1680 - val_accuracy: 0.1253\n",
      "Epoch 230/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1228 - accuracy: 0.2455 - val_loss: 1.1678 - val_accuracy: 0.1264\n",
      "Epoch 231/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1228 - accuracy: 0.2440 - val_loss: 1.1680 - val_accuracy: 0.1292\n",
      "Epoch 232/300\n",
      "5371/5371 [==============================] - 0s 64us/sample - loss: 1.1228 - accuracy: 0.2445 - val_loss: 1.1683 - val_accuracy: 0.1197\n",
      "Epoch 233/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1228 - accuracy: 0.2450 - val_loss: 1.1680 - val_accuracy: 0.1247\n",
      "Epoch 234/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1227 - accuracy: 0.2456 - val_loss: 1.1681 - val_accuracy: 0.1230\n",
      "Epoch 235/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1227 - accuracy: 0.2449 - val_loss: 1.1679 - val_accuracy: 0.1247\n",
      "Epoch 236/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1229 - accuracy: 0.2451 - val_loss: 1.1685 - val_accuracy: 0.1303\n",
      "Epoch 237/300\n",
      "5371/5371 [==============================] - 0s 75us/sample - loss: 1.1228 - accuracy: 0.2458 - val_loss: 1.1683 - val_accuracy: 0.1298\n",
      "Epoch 238/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1228 - accuracy: 0.2469 - val_loss: 1.1683 - val_accuracy: 0.1258\n",
      "Epoch 239/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1232 - accuracy: 0.2417 - val_loss: 1.1686 - val_accuracy: 0.1286\n",
      "Epoch 240/300\n",
      "5371/5371 [==============================] - 0s 64us/sample - loss: 1.1232 - accuracy: 0.2436 - val_loss: 1.1685 - val_accuracy: 0.1236\n",
      "Epoch 241/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1229 - accuracy: 0.2456 - val_loss: 1.1685 - val_accuracy: 0.1264\n",
      "Epoch 242/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1227 - accuracy: 0.2442 - val_loss: 1.1687 - val_accuracy: 0.1197\n",
      "Epoch 243/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1227 - accuracy: 0.2442 - val_loss: 1.1685 - val_accuracy: 0.1270\n",
      "Epoch 244/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1226 - accuracy: 0.2458 - val_loss: 1.1686 - val_accuracy: 0.1230\n",
      "Epoch 245/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1226 - accuracy: 0.2453 - val_loss: 1.1686 - val_accuracy: 0.1230\n",
      "Epoch 246/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1226 - accuracy: 0.2461 - val_loss: 1.1686 - val_accuracy: 0.1258\n",
      "Epoch 247/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1225 - accuracy: 0.2449 - val_loss: 1.1687 - val_accuracy: 0.1258\n",
      "Epoch 248/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1226 - accuracy: 0.2469 - val_loss: 1.1685 - val_accuracy: 0.1219\n",
      "Epoch 249/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1225 - accuracy: 0.2472 - val_loss: 1.1683 - val_accuracy: 0.1242\n",
      "Epoch 250/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1225 - accuracy: 0.2471 - val_loss: 1.1683 - val_accuracy: 0.1258\n",
      "Epoch 251/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1226 - accuracy: 0.2453 - val_loss: 1.1692 - val_accuracy: 0.1270\n",
      "Epoch 252/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1225 - accuracy: 0.2464 - val_loss: 1.1689 - val_accuracy: 0.1180\n",
      "Epoch 253/300\n",
      "5371/5371 [==============================] - 0s 71us/sample - loss: 1.1225 - accuracy: 0.2461 - val_loss: 1.1688 - val_accuracy: 0.1208\n",
      "Epoch 254/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1224 - accuracy: 0.2474 - val_loss: 1.1685 - val_accuracy: 0.1264\n",
      "Epoch 255/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1224 - accuracy: 0.2469 - val_loss: 1.1685 - val_accuracy: 0.1230\n",
      "Epoch 256/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1224 - accuracy: 0.2497 - val_loss: 1.1687 - val_accuracy: 0.1275\n",
      "Epoch 257/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1224 - accuracy: 0.2476 - val_loss: 1.1688 - val_accuracy: 0.1186\n",
      "Epoch 258/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1224 - accuracy: 0.2466 - val_loss: 1.1688 - val_accuracy: 0.1236\n",
      "Epoch 259/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1224 - accuracy: 0.2488 - val_loss: 1.1685 - val_accuracy: 0.1158\n",
      "Epoch 260/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1224 - accuracy: 0.2470 - val_loss: 1.1687 - val_accuracy: 0.1169\n",
      "Epoch 261/300\n",
      "5371/5371 [==============================] - 0s 64us/sample - loss: 1.1224 - accuracy: 0.2483 - val_loss: 1.1689 - val_accuracy: 0.1247\n",
      "Epoch 262/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1223 - accuracy: 0.2489 - val_loss: 1.1688 - val_accuracy: 0.1258\n",
      "Epoch 263/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1224 - accuracy: 0.2491 - val_loss: 1.1690 - val_accuracy: 0.1236\n",
      "Epoch 264/300\n",
      "5371/5371 [==============================] - 0s 73us/sample - loss: 1.1223 - accuracy: 0.2482 - val_loss: 1.1687 - val_accuracy: 0.1247\n",
      "Epoch 265/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1223 - accuracy: 0.2471 - val_loss: 1.1687 - val_accuracy: 0.1247\n",
      "Epoch 266/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1223 - accuracy: 0.2486 - val_loss: 1.1686 - val_accuracy: 0.1270\n",
      "Epoch 267/300\n",
      "5371/5371 [==============================] - 0s 65us/sample - loss: 1.1222 - accuracy: 0.2481 - val_loss: 1.1687 - val_accuracy: 0.1264\n",
      "Epoch 268/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1222 - accuracy: 0.2492 - val_loss: 1.1687 - val_accuracy: 0.1275\n",
      "Epoch 269/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1223 - accuracy: 0.2496 - val_loss: 1.1691 - val_accuracy: 0.1275\n",
      "Epoch 270/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1223 - accuracy: 0.2482 - val_loss: 1.1692 - val_accuracy: 0.1230\n",
      "Epoch 271/300\n",
      "5371/5371 [==============================] - 0s 69us/sample - loss: 1.1222 - accuracy: 0.2477 - val_loss: 1.1696 - val_accuracy: 0.1242\n",
      "Epoch 272/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5371/5371 [==============================] - 0s 66us/sample - loss: 1.1222 - accuracy: 0.2482 - val_loss: 1.1693 - val_accuracy: 0.1242\n",
      "Epoch 273/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1222 - accuracy: 0.2490 - val_loss: 1.1694 - val_accuracy: 0.1298\n",
      "Epoch 274/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1222 - accuracy: 0.2479 - val_loss: 1.1694 - val_accuracy: 0.1247\n",
      "Epoch 275/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1223 - accuracy: 0.2483 - val_loss: 1.1695 - val_accuracy: 0.1270\n",
      "Epoch 276/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1222 - accuracy: 0.2493 - val_loss: 1.1692 - val_accuracy: 0.1186\n",
      "Epoch 277/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1221 - accuracy: 0.2478 - val_loss: 1.1694 - val_accuracy: 0.1236\n",
      "Epoch 278/300\n",
      "5371/5371 [==============================] - 0s 67us/sample - loss: 1.1221 - accuracy: 0.2479 - val_loss: 1.1696 - val_accuracy: 0.1219\n",
      "Epoch 279/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1221 - accuracy: 0.2497 - val_loss: 1.1690 - val_accuracy: 0.1230\n",
      "Epoch 280/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1221 - accuracy: 0.2493 - val_loss: 1.1693 - val_accuracy: 0.1242\n",
      "Epoch 281/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1220 - accuracy: 0.2485 - val_loss: 1.1692 - val_accuracy: 0.1202\n",
      "Epoch 282/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1220 - accuracy: 0.2479 - val_loss: 1.1693 - val_accuracy: 0.1253\n",
      "Epoch 283/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1223 - accuracy: 0.2464 - val_loss: 1.1698 - val_accuracy: 0.1225\n",
      "Epoch 284/300\n",
      "5371/5371 [==============================] - 1s 103us/sample - loss: 1.1226 - accuracy: 0.2478 - val_loss: 1.1700 - val_accuracy: 0.1186\n",
      "Epoch 285/300\n",
      "5371/5371 [==============================] - 1s 211us/sample - loss: 1.1223 - accuracy: 0.2462 - val_loss: 1.1700 - val_accuracy: 0.1242\n",
      "Epoch 286/300\n",
      "5371/5371 [==============================] - 0s 64us/sample - loss: 1.1221 - accuracy: 0.2489 - val_loss: 1.1700 - val_accuracy: 0.1236\n",
      "Epoch 287/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1222 - accuracy: 0.2485 - val_loss: 1.1700 - val_accuracy: 0.1253\n",
      "Epoch 288/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1221 - accuracy: 0.2493 - val_loss: 1.1701 - val_accuracy: 0.1247\n",
      "Epoch 289/300\n",
      "5371/5371 [==============================] - 0s 59us/sample - loss: 1.1222 - accuracy: 0.2500 - val_loss: 1.1700 - val_accuracy: 0.1225\n",
      "Epoch 290/300\n",
      "5371/5371 [==============================] - 0s 60us/sample - loss: 1.1221 - accuracy: 0.2497 - val_loss: 1.1699 - val_accuracy: 0.1163\n",
      "Epoch 291/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1220 - accuracy: 0.2503 - val_loss: 1.1695 - val_accuracy: 0.1230\n",
      "Epoch 292/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1220 - accuracy: 0.2513 - val_loss: 1.1693 - val_accuracy: 0.1225\n",
      "Epoch 293/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1219 - accuracy: 0.2501 - val_loss: 1.1694 - val_accuracy: 0.1275\n",
      "Epoch 294/300\n",
      "5371/5371 [==============================] - 0s 72us/sample - loss: 1.1219 - accuracy: 0.2521 - val_loss: 1.1696 - val_accuracy: 0.1230\n",
      "Epoch 295/300\n",
      "5371/5371 [==============================] - 0s 63us/sample - loss: 1.1220 - accuracy: 0.2515 - val_loss: 1.1695 - val_accuracy: 0.1174\n",
      "Epoch 296/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1219 - accuracy: 0.2510 - val_loss: 1.1694 - val_accuracy: 0.1247\n",
      "Epoch 297/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1219 - accuracy: 0.2499 - val_loss: 1.1697 - val_accuracy: 0.1253\n",
      "Epoch 298/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1219 - accuracy: 0.2509 - val_loss: 1.1696 - val_accuracy: 0.1264\n",
      "Epoch 299/300\n",
      "5371/5371 [==============================] - 0s 61us/sample - loss: 1.1218 - accuracy: 0.2513 - val_loss: 1.1696 - val_accuracy: 0.1253\n",
      "Epoch 300/300\n",
      "5371/5371 [==============================] - 0s 74us/sample - loss: 1.1219 - accuracy: 0.2537 - val_loss: 1.1693 - val_accuracy: 0.1230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26d685796c8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=300,batch_size=100,validation_data = (x_val,y_val),callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_matrix = model.predict(np.array(list(paper_table['title_vector'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, node in enumerate(paper_nodes):\n",
    "    node['predict_vector'] = predict_matrix[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2555, 2555)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix = np.zeros((len(paper_nodes),len(paper_nodes)))\n",
    "for i, nodeA in enumerate(paper_nodes):\n",
    "    for j, nodeB in enumerate(paper_nodes):\n",
    "        if i is not j:\n",
    "            distance_matrix[i,j] = np.linalg.norm(np.dot(nodeA['title_vector'],nodeB['title_vector'].T))\n",
    "max_distance = np.max(distance_matrix)\n",
    "distance_matrix = distance_matrix/max_distance\n",
    "distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id_a</th>\n",
       "      <th>paper_id_b</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987</td>\n",
       "      <td>1987</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987</td>\n",
       "      <td>316</td>\n",
       "      <td>0.261576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987</td>\n",
       "      <td>358</td>\n",
       "      <td>0.146118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1987</td>\n",
       "      <td>2227</td>\n",
       "      <td>0.262546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987</td>\n",
       "      <td>2437</td>\n",
       "      <td>0.230848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528020</th>\n",
       "      <td>1169</td>\n",
       "      <td>157</td>\n",
       "      <td>0.169219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528021</th>\n",
       "      <td>1169</td>\n",
       "      <td>101</td>\n",
       "      <td>0.242356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528022</th>\n",
       "      <td>1169</td>\n",
       "      <td>395</td>\n",
       "      <td>0.215330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528023</th>\n",
       "      <td>1169</td>\n",
       "      <td>1914</td>\n",
       "      <td>0.227054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528024</th>\n",
       "      <td>1169</td>\n",
       "      <td>1169</td>\n",
       "      <td>0.263842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6528025 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         paper_id_a  paper_id_b  distance\n",
       "0              1987        1987  0.000000\n",
       "1              1987         316  0.261576\n",
       "2              1987         358  0.146118\n",
       "3              1987        2227  0.262546\n",
       "4              1987        2437  0.230848\n",
       "...             ...         ...       ...\n",
       "6528020        1169         157  0.169219\n",
       "6528021        1169         101  0.242356\n",
       "6528022        1169         395  0.215330\n",
       "6528023        1169        1914  0.227054\n",
       "6528024        1169        1169  0.263842\n",
       "\n",
       "[6528025 rows x 3 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_table = list()\n",
    "for i, nodeA in enumerate(paper_nodes):\n",
    "    for j, nodeB in enumerate(paper_nodes):\n",
    "        temp = dict()\n",
    "        temp['paper_id_a'] = nodeA['paper.id']\n",
    "        temp['paper_id_b'] = nodeB['paper.id']\n",
    "        temp['distance'] = distance_matrix[i,j]\n",
    "        distance_table.append(temp)\n",
    "distance_table = pd.DataFrame(distance_table)\n",
    "distance_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_filter(node_id):\n",
    "    table = distance_table[distance_table[\"paper_id_a\"] == node_id] #select\n",
    "    table = table.sort_values(by='distance') #sort \n",
    "    table = table.head(10) #top 10\n",
    "    return list(table['paper_id_b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Paper 2554\n",
    "The title of paper `2554` is `The Efficient Retrieval of Partial Documents`\n",
    "\n",
    "With Chinese meaning `对部分文档的有效检索`\n",
    "\n",
    "### The 10 articles with the most similar titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'node.title': 'Erweiterbarkeit, Kooperation, Foderation von Datenbanksystemen'}] 2405\n",
      "[{'node.title': 'Transaktionsunterstutzung fur Workflows'}] 93\n",
      "[{'node.title': 'Comparing Subsumption Optimizations'}] 340\n",
      "[{'node.title': 'Explaining ALC Subsumption'}] 345\n",
      "[{'node.title': 'D-Tree Substitution Grammars'}] 1119\n",
      "[{'node.title': 'Dynamic Classifier Selection'}] 941\n",
      "[{'node.title': 'Bidirectional Contextual Resolution'}] 1096\n",
      "[{'node.title': 'The Generative Lexicon'}] 1087\n",
      "[{'node.title': 'Classifier Instability and Partitioning'}] 881\n",
      "[{'node.title': 'Reusing Analogous Components'}] 69\n"
     ]
    }
   ],
   "source": [
    "for i in distance_filter(2554):\n",
    "    print(graph.run(\"MATCH (node:paper{id: %d}) RETURN node.title\" % i).data(),i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Erweiterbarkeit, Kooperation, Foderation von Datenbanksystemen `2405` `可扩充、合作、创造档案`\n",
    "2. Transaktionsunterstutzung fur Workflows `93` `Transaktionsunterstutzung毛皮工作流程`\n",
    "3. Comparing Subsumption Optimizations `340` `比较包容优化`\n",
    "4. Explaining ALC Subsumption `345` `解释ALC包含`\n",
    "5. D-Tree Substitution Grammars `1119` `D-树替换语法`\n",
    "6. Dynamic Classifier Selection `941` `动态分类器选择`\n",
    "7. Bidirectional Contextual Resolution `1096` `双向上下文分辨`\n",
    "8. The Generative Lexicon `1087` `生成词汇`\n",
    "9. Classifier Instability and Partitioning `881` `分类器不稳定性和划分`\n",
    "10. Reusing Analogous Components `69` `重复使用类似组件`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper cited `2554` or paper is cited by `2554` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, target]\n",
       "Index: []"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_table[citation_table['source'] == 2554] #no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>2522</td>\n",
       "      <td>2554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source  target\n",
       "4036    2522    2554"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_table[citation_table['target'] == 2554] #2554 cited 2522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'node.title': 'Fast Ranking in Limited Space'}] 2522\n"
     ]
    }
   ],
   "source": [
    "print(graph.run(\"MATCH (node:paper{id: %d}) RETURN node.title\" % 2522).data(),2522)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fast Ranking in Limited Space  `2522` `有限空间快速排序`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
